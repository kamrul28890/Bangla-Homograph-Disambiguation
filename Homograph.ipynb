{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.1-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/124.9 MB 1.3 MB/s eta 0:01:38\n",
      "   ---------------------------------------- 0.5/124.9 MB 7.0 MB/s eta 0:00:18\n",
      "   ---------------------------------------- 1.3/124.9 MB 13.4 MB/s eta 0:00:10\n",
      "    --------------------------------------- 2.1/124.9 MB 16.5 MB/s eta 0:00:08\n",
      "    --------------------------------------- 2.9/124.9 MB 16.6 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 3.6/124.9 MB 17.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 4.5/124.9 MB 18.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 5.2/124.9 MB 18.6 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 6.0/124.9 MB 19.2 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 6.8/124.9 MB 19.8 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 7.6/124.9 MB 20.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 8.4/124.9 MB 19.8 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 9.2/124.9 MB 20.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 10.0/124.9 MB 20.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 10.8/124.9 MB 21.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 11.6/124.9 MB 21.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 12.4/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 13.1/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 13.9/124.9 MB 21.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 14.7/124.9 MB 21.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 15.5/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 16.3/124.9 MB 21.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 17.0/124.9 MB 21.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 17.8/124.9 MB 21.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 18.6/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 19.4/124.9 MB 21.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 20.2/124.9 MB 21.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 21.0/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 21.8/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 22.6/124.9 MB 21.8 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 23.4/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 24.2/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 25.0/124.9 MB 21.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 25.8/124.9 MB 21.9 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 26.6/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 27.3/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 28.1/124.9 MB 21.8 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 28.9/124.9 MB 21.8 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 29.7/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 30.5/124.9 MB 22.5 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 31.3/124.9 MB 21.9 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 32.0/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 32.9/124.9 MB 22.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 33.7/124.9 MB 21.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 34.4/124.9 MB 21.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 35.2/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.0/124.9 MB 22.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.8/124.9 MB 21.9 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 37.6/124.9 MB 21.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 38.3/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 39.1/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 40.0/124.9 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 40.7/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 41.5/124.9 MB 22.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 42.3/124.9 MB 21.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 43.1/124.9 MB 21.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 43.9/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 44.7/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 45.4/124.9 MB 21.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 46.2/124.9 MB 21.9 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 47.0/124.9 MB 22.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 47.8/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 48.6/124.9 MB 21.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 49.4/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 50.2/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 50.9/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 51.7/124.9 MB 21.9 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 52.5/124.9 MB 22.5 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 53.3/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 54.1/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 54.9/124.9 MB 21.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 55.7/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 56.4/124.9 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 57.2/124.9 MB 21.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 58.0/124.9 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 58.8/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 59.6/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 60.4/124.9 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 61.2/124.9 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 62.0/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 62.7/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 63.5/124.9 MB 21.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 64.3/124.9 MB 21.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 65.1/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 65.8/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 66.6/124.9 MB 21.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 67.4/124.9 MB 21.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 68.2/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 69.0/124.9 MB 21.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 69.8/124.9 MB 21.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 70.6/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 71.4/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 72.1/124.9 MB 21.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 72.9/124.9 MB 21.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 73.7/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.5/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 75.3/124.9 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 76.1/124.9 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 76.8/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 77.6/124.9 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 78.5/124.9 MB 21.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 79.2/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 80.0/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 80.8/124.9 MB 21.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 81.6/124.9 MB 21.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 82.4/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 83.2/124.9 MB 22.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 84.0/124.9 MB 21.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 84.7/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 85.5/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 86.3/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 87.1/124.9 MB 21.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 87.9/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 88.7/124.9 MB 22.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 89.5/124.9 MB 21.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 90.3/124.9 MB 21.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 91.1/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 91.9/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 92.6/124.9 MB 21.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 93.4/124.9 MB 21.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 94.2/124.9 MB 22.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 95.0/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 95.8/124.9 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 96.6/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 97.4/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 98.2/124.9 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 99.0/124.9 MB 21.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 99.8/124.9 MB 22.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 100.6/124.9 MB 21.9 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 101.4/124.9 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 102.2/124.9 MB 22.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 102.9/124.9 MB 22.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 103.8/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 104.6/124.9 MB 22.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 105.4/124.9 MB 22.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 106.2/124.9 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 106.9/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 107.7/124.9 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 108.5/124.9 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 109.3/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 110.1/124.9 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 110.9/124.9 MB 22.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 111.7/124.9 MB 21.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 112.5/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 113.2/124.9 MB 22.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 114.0/124.9 MB 22.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 114.8/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 115.6/124.9 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 116.4/124.9 MB 22.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 117.2/124.9 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 117.9/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 118.7/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 119.6/124.9 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 120.4/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 121.1/124.9 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  121.9/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  122.7/124.9 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  123.5/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.3/124.9 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 13.1 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.1\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\n",
      "   ---------------------------------------- 0.0/258.3 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/258.3 kB 960.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 258.3/258.3 kB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_tts_wsd\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\bol_bolo\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\dak_dako\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\kal_kalo\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\komol_komlo\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\mot_moto\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_tts_wsd\\scripts\n",
      "\n",
      "File Paths\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph.zip\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged.rar\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\cc.bn.300.vec.gz\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\clean_corpus.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\homograph_word.docx\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\homograph_word.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\M1_presentation.ppt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\main.ipynb\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\main2.ipynb\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\bol_bolo\\Bol.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\bol_bolo\\Bolo.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\bol_bolo\\bolo_after.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\bol_bolo\\bolo_before.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\bol_bolo\\bol_after.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\bol_bolo\\bol_before.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\dak_dako\\dak.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\dak_dako\\dako.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\dak_dako\\dako_after.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\dak_dako\\dako_before.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\dak_dako\\dak_after.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\dak_dako\\dak_before.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\kal_kalo\\Kal.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\kal_kalo\\kalo.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\kal_kalo\\kalo_after.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\kal_kalo\\kalo_before.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\kal_kalo\\kal_after.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\kal_kalo\\kal_before.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\komol_komlo\\komlo.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\komol_komlo\\komol.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\mot_moto\\mot.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\mot_moto\\moto.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\mot_moto\\moto_after.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\mot_moto\\moto_before.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\mot_moto\\mot_after.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\mot_moto\\mot_before.txt\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_tts_wsd\\main.py\n",
      "C:\\Users\\kamru\\Documents\\Homograph\\bangla_tts_wsd\\scripts\\data_loader.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = r\"C:\\Users\\kamru\\Documents\\Homograph\"\n",
    "\n",
    "# Initialize lists to store directories and file paths\n",
    "directories = []\n",
    "file_paths = []\n",
    "\n",
    "#Walk through the directory and get directories and files\n",
    "\n",
    "for root, dirs, files in os.walk(directory_path):\n",
    "    #Add directories to the list\n",
    "    for dir_name in dirs:\n",
    "        directories.append(os.path.join(root, dir_name))\n",
    "\n",
    "    #Add file paths to the list\n",
    "    for file_name in files:\n",
    "        file_paths.append(os.path.join(root, file_name))\n",
    "\n",
    "#Print all directories\n",
    "print(\"Directories\")\n",
    "for dir_path in directories:\n",
    "    print(dir_path)\n",
    "\n",
    "#Print all file path\n",
    "print(\"\\nFile Paths\")\n",
    "for file_path in file_paths:\n",
    "    print(file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp310-cp310-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.0 MB 2.0 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.7/11.0 MB 11.6 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.5/11.0 MB 13.8 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.3/11.0 MB 16.2 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 17.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.8/11.0 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.6/11.0 MB 18.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.4/11.0 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.2/11.0 MB 19.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.0/11.0 MB 20.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.8/11.0 MB 20.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.6/11.0 MB 20.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.4/11.0 MB 20.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.1/11.0 MB 20.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.9/11.0 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 21.1 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Function to read and preprocess text files\n",
    "def read_and_preprocess(file_path, label):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    # Remove extra spaces and add a label\n",
    "    data = [(line.strip(), label) for line in lines if line.strip()]\n",
    "    return data\n",
    "\n",
    "# Function to combine multiple labeled datasets into a single DataFrame\n",
    "def combine_data(*data_sets):\n",
    "    combined_data = []\n",
    "    for data in data_sets:\n",
    "        combined_data.extend(data)\n",
    "    return pd.DataFrame(combined_data, columns=['text', 'label'])\n",
    "\n",
    "# Function to vectorize text data using TF-IDF\n",
    "def vectorize_text(data_frame):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(data_frame['text'])\n",
    "    return X, data_frame['label'], vectorizer\n",
    "\n",
    "# Function to train a Naive Bayes model\n",
    "def train_naive_bayes(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model, X_test, y_test\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    return accuracy, conf_matrix, class_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use double backslashes for file paths\n",
    "dak_data = read_and_preprocess(\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\dak_dako\\\\dak.txt\", 'dak')\n",
    "dako_data = read_and_preprocess(\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\dak_dako\\\\dako.txt\", 'dako')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dak and dako data into a single DataFrame\n",
    "df = combine_data(dak_data, dako_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using TF-IDF\n",
    "X, y, vectorizer = vectorize_text(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes model on the vectorized data\n",
    "model, X_test, y_test = train_naive_bayes(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8212121212121212\n",
      "Confusion Matrix:\n",
      " [[259   0]\n",
      " [ 59  12]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         dak       0.81      1.00      0.90       259\n",
      "        dako       1.00      0.17      0.29        71\n",
      "\n",
      "    accuracy                           0.82       330\n",
      "   macro avg       0.91      0.58      0.59       330\n",
      "weighted avg       0.85      0.82      0.77       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print the results\n",
    "accuracy, conf_matrix, class_report = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Code Block for bol_bolo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bol_Bolo Model Accuracy: 0.9112021857923497\n",
      "Bol_Bolo Confusion Matrix:\n",
      " [[657   0]\n",
      " [ 65  10]]\n",
      "Bol_Bolo Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bol       0.91      1.00      0.95       657\n",
      "        bolo       1.00      0.13      0.24        75\n",
      "\n",
      "    accuracy                           0.91       732\n",
      "   macro avg       0.95      0.57      0.59       732\n",
      "weighted avg       0.92      0.91      0.88       732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading and Preprocessing Bol and Bolo Data\n",
    "bol_path = r\"C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\bol_bolo\\Bol.txt\"\n",
    "bolo_path = r\"C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\bol_bolo\\Bolo.txt\"\n",
    "\n",
    "bol_data = read_and_preprocess(bol_path, 'bol')\n",
    "bolo_data = read_and_preprocess(bolo_path, 'bolo')\n",
    "\n",
    "# Combine Bol and Bolo Data\n",
    "df_bol_bolo = combine_data(bol_data, bolo_data)\n",
    "\n",
    "# Vectorize the Text Data\n",
    "X_bol_bolo, y_bol_bolo, vectorizer_bol_bolo = vectorize_text(df_bol_bolo)\n",
    "\n",
    "# Train the Naive Bayes Model\n",
    "model_bol_bolo, X_test_bol_bolo, y_test_bol_bolo = train_naive_bayes(X_bol_bolo, y_bol_bolo)\n",
    "\n",
    "# Evaluate the Model\n",
    "accuracy_bol_bolo, conf_matrix_bol_bolo, class_report_bol_bolo = evaluate_model(model_bol_bolo, X_test_bol_bolo, y_test_bol_bolo)\n",
    "\n",
    "# Print Results\n",
    "print(\"Bol_Bolo Model Accuracy:\", accuracy_bol_bolo)\n",
    "print(\"Bol_Bolo Confusion Matrix:\\n\", conf_matrix_bol_bolo)\n",
    "print(\"Bol_Bolo Classification Report:\\n\", class_report_bol_bolo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kal_Kalo Model Accuracy: 0.9312080536912751\n",
      "Kal_Kalo Confusion Matrix:\n",
      " [[1105    0]\n",
      " [  82    5]]\n",
      "Kal_Kalo Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         kal       0.93      1.00      0.96      1105\n",
      "        kalo       1.00      0.06      0.11        87\n",
      "\n",
      "    accuracy                           0.93      1192\n",
      "   macro avg       0.97      0.53      0.54      1192\n",
      "weighted avg       0.94      0.93      0.90      1192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and define functions (if not already imported/defined)\n",
    "\n",
    "# Reading and Preprocessing Kal and Kalo Data\n",
    "kal_path = r\"C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\kal_kalo\\Kal.txt\"\n",
    "kalo_path = r\"C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\kal_kalo\\kalo.txt\"\n",
    "\n",
    "kal_data = read_and_preprocess(kal_path, 'kal')\n",
    "kalo_data = read_and_preprocess(kalo_path, 'kalo')\n",
    "\n",
    "# Combine Kal and Kalo Data\n",
    "df_kal_kalo = combine_data(kal_data, kalo_data)\n",
    "\n",
    "# Vectorize the Text Data\n",
    "X_kal_kalo, y_kal_kalo, vectorizer_kal_kalo = vectorize_text(df_kal_kalo)\n",
    "\n",
    "# Train the Naive Bayes Model\n",
    "model_kal_kalo, X_test_kal_kalo, y_test_kal_kalo = train_naive_bayes(X_kal_kalo, y_kal_kalo)\n",
    "\n",
    "# Evaluate the Model\n",
    "accuracy_kal_kalo, conf_matrix_kal_kalo, class_report_kal_kalo = evaluate_model(model_kal_kalo, X_test_kal_kalo, y_test_kal_kalo)\n",
    "\n",
    "# Print Results\n",
    "print(\"Kal_Kalo Model Accuracy:\", accuracy_kal_kalo)\n",
    "print(\"Kal_Kalo Confusion Matrix:\\n\", conf_matrix_kal_kalo)\n",
    "print(\"Kal_Kalo Classification Report:\\n\", class_report_kal_kalo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komol_Komlo Model Accuracy: 0.8828828828828829\n",
      "Komol_Komlo Confusion Matrix:\n",
      " [[73  0]\n",
      " [13 25]]\n",
      "Komol_Komlo Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       komlo       0.85      1.00      0.92        73\n",
      "       komol       1.00      0.66      0.79        38\n",
      "\n",
      "    accuracy                           0.88       111\n",
      "   macro avg       0.92      0.83      0.86       111\n",
      "weighted avg       0.90      0.88      0.88       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and define functions (if not already imported/defined)\n",
    "\n",
    "# Reading and Preprocessing Komol and Komlo Data\n",
    "komol_path = r\"C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\komol_komlo\\komol.txt\"\n",
    "komlo_path = r\"C:\\Users\\kamru\\Documents\\Homograph\\bangla_homograph_tagged\\komol_komlo\\komlo.txt\"\n",
    "\n",
    "komol_data = read_and_preprocess(komol_path, 'komol')\n",
    "komlo_data = read_and_preprocess(komlo_path, 'komlo')\n",
    "\n",
    "# Combine Komol and Komlo Data\n",
    "df_komol_komlo = combine_data(komol_data, komlo_data)\n",
    "\n",
    "# Vectorize the Text Data\n",
    "X_komol_komlo, y_komol_komlo, vectorizer_komol_komlo = vectorize_text(df_komol_komlo)\n",
    "\n",
    "# Train the Naive Bayes Model\n",
    "model_komol_komlo, X_test_komol_komlo, y_test_komol_komlo = train_naive_bayes(X_komol_komlo, y_komol_komlo)\n",
    "\n",
    "# Evaluate the Model\n",
    "accuracy_komol_komlo, conf_matrix_komol_komlo, class_report_komol_komlo = evaluate_model(model_komol_komlo, X_test_komol_komlo, y_test_komol_komlo)\n",
    "\n",
    "# Print Results\n",
    "print(\"Komol_Komlo Model Accuracy:\", accuracy_komol_komlo)\n",
    "print(\"Komol_Komlo Confusion Matrix:\\n\", conf_matrix_komol_komlo)\n",
    "print(\"Komol_Komlo Classification Report:\\n\", class_report_komol_komlo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mot_Moto Resampled Model Accuracy: 0.71875\n",
      "Mot_Moto Resampled Confusion Matrix:\n",
      " [[11  5]\n",
      " [ 4 12]]\n",
      "Mot_Moto Resampled Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         mot       0.73      0.69      0.71        16\n",
      "        moto       0.71      0.75      0.73        16\n",
      "\n",
      "    accuracy                           0.72        32\n",
      "   macro avg       0.72      0.72      0.72        32\n",
      "weighted avg       0.72      0.72      0.72        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Define the evaluate_model_zero_division function (if not defined already)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_model_zero_division(model, X_test, y_test):\n",
    "    \"\"\"Evaluates the model and returns accuracy, confusion matrix, and classification report.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, zero_division=0)  # Use zero_division to handle undefined metrics\n",
    "    return accuracy, conf_matrix, class_report\n",
    "\n",
    "# Re-run the code after defining the function\n",
    "accuracy_mot_moto_resampled, conf_matrix_mot_moto_resampled, class_report_mot_moto_resampled = evaluate_model_zero_division(model_mot_moto_resampled, X_test_mot_moto_resampled, y_test_mot_moto_resampled)\n",
    "\n",
    "# Print results for the resampled model\n",
    "print(\"Mot_Moto Resampled Model Accuracy:\", accuracy_mot_moto_resampled)\n",
    "print(\"Mot_Moto Resampled Confusion Matrix:\\n\", conf_matrix_mot_moto_resampled)\n",
    "print(\"Mot_Moto Resampled Classification Report:\\n\", class_report_mot_moto_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and Preprocessing Dak and Dako Data\n",
    "dak_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\dak_dako\\\\dak.txt\", 'dak')\n",
    "dako_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\dak_dako\\\\dako.txt\", 'dako')\n",
    "\n",
    "# Combine Dak and Dako Data\n",
    "df_dak_dako = combine_data(dak_data, dako_data)\n",
    "\n",
    "# Vectorize the Text Data for Dak_Dako\n",
    "X_dak_dako, y_dak_dako, vectorizer_dak_dako = vectorize_text(df_dak_dako)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reading and Preprocessing Bol and Bolo Data\n",
    "bol_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\bol_bolo\\\\Bol.txt\", 'bol')\n",
    "bolo_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\bol_bolo\\\\Bolo.txt\", 'bolo')\n",
    "\n",
    "# Combine Bol and Bolo Data\n",
    "df_bol_bolo = combine_data(bol_data, bolo_data)\n",
    "\n",
    "# Vectorize the Text Data for Bol_Bolo\n",
    "X_bol_bolo, y_bol_bolo, vectorizer_bol_bolo = vectorize_text(df_bol_bolo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reading and Preprocessing Kal and Kalo Data\n",
    "kal_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\kal_kalo\\\\Kal.txt\", 'kal')\n",
    "kalo_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\kal_kalo\\\\kalo.txt\", 'kalo')\n",
    "\n",
    "# Combine Kal and Kalo Data\n",
    "df_kal_kalo = combine_data(kal_data, kalo_data)\n",
    "\n",
    "# Vectorize the Text Data for Kal_Kalo\n",
    "X_kal_kalo, y_kal_kalo, vectorizer_kal_kalo = vectorize_text(df_kal_kalo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reading and Preprocessing Komol and Komlo Data\n",
    "komol_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\komol_komlo\\\\komol.txt\", 'komol')\n",
    "komlo_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\komol_komlo\\\\komlo.txt\", 'komlo')\n",
    "\n",
    "# Combine Komol and Komlo Data\n",
    "df_komol_komlo = combine_data(komol_data, komlo_data)\n",
    "\n",
    "# Vectorize the Text Data for Komol_Komlo\n",
    "X_komol_komlo, y_komol_komlo, vectorizer_komol_komlo = vectorize_text(df_komol_komlo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reading and Preprocessing Mot and Moto Data\n",
    "mot_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\mot_moto\\\\mot.txt\", 'mot')\n",
    "moto_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\mot_moto\\\\moto.txt\", 'moto')\n",
    "\n",
    "# Combine Mot and Moto Data\n",
    "df_mot_moto = combine_data(mot_data, moto_data)\n",
    "\n",
    "# Vectorize the Text Data for Mot_Moto\n",
    "X_mot_moto, y_mot_moto, vectorizer_mot_moto = vectorize_text(df_mot_moto)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train and evaluate an SVM model\n",
    "def train_evaluate_svm(X, y, homograph_name):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # Train SVM model\n",
    "    model_svm = SVC(kernel='linear', random_state=42, class_weight='balanced')\n",
    "    model_svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model_svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # Print the evaluation results\n",
    "    print(f\"{homograph_name} SVM Model Accuracy: {accuracy}\")\n",
    "    print(f\"{homograph_name} SVM Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(f\"{homograph_name} SVM Classification Report:\\n\", class_report)\n",
    "    \n",
    "    # Return model and test data for further evaluation if needed\n",
    "    return model_svm, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and preprocess text files\n",
    "def read_and_preprocess(file_path, label):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    data = [(line.strip(), label) for line in lines if line.strip()]\n",
    "    return data\n",
    "\n",
    "# Function to combine multiple labeled datasets into a single DataFrame\n",
    "def combine_data(*data_sets):\n",
    "    combined_data = []\n",
    "    for data in data_sets:\n",
    "        combined_data.extend(data)\n",
    "    return pd.DataFrame(combined_data, columns=['text', 'label'])\n",
    "\n",
    "# Function to vectorize text data using TF-IDF\n",
    "def vectorize_text(data_frame):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(data_frame['text'])\n",
    "    return X, data_frame['label'], vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dak_Dako\n",
    "dak_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\dak_dako\\\\dak.txt\", 'dak')\n",
    "dako_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\dak_dako\\\\dako.txt\", 'dako')\n",
    "df_dak_dako = combine_data(dak_data, dako_data)\n",
    "X_dak_dako, y_dak_dako, vectorizer_dak_dako = vectorize_text(df_dak_dako)\n",
    "\n",
    "# Bol_Bolo\n",
    "bol_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\bol_bolo\\\\Bol.txt\", 'bol')\n",
    "bolo_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\bol_bolo\\\\Bolo.txt\", 'bolo')\n",
    "df_bol_bolo = combine_data(bol_data, bolo_data)\n",
    "X_bol_bolo, y_bol_bolo, vectorizer_bol_bolo = vectorize_text(df_bol_bolo)\n",
    "\n",
    "# Kal_Kalo\n",
    "kal_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\kal_kalo\\\\Kal.txt\", 'kal')\n",
    "kalo_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\kal_kalo\\\\kalo.txt\", 'kalo')\n",
    "df_kal_kalo = combine_data(kal_data, kalo_data)\n",
    "X_kal_kalo, y_kal_kalo, vectorizer_kal_kalo = vectorize_text(df_kal_kalo)\n",
    "\n",
    "# Komol_Komlo\n",
    "komol_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\komol_komlo\\\\komol.txt\", 'komol')\n",
    "komlo_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\komol_komlo\\\\komlo.txt\", 'komlo')\n",
    "df_komol_komlo = combine_data(komol_data, komlo_data)\n",
    "X_komol_komlo, y_komol_komlo, vectorizer_komol_komlo = vectorize_text(df_komol_komlo)\n",
    "\n",
    "# Mot_Moto\n",
    "mot_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\mot_moto\\\\mot.txt\", 'mot')\n",
    "moto_data = read_and_preprocess(r\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\mot_moto\\\\moto.txt\", 'moto')\n",
    "df_mot_moto = combine_data(mot_data, moto_data)\n",
    "X_mot_moto, y_mot_moto, vectorizer_mot_moto = vectorize_text(df_mot_moto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dak_Dako SVM Model Accuracy: 0.9333333333333333\n",
      "Dak_Dako SVM Confusion Matrix:\n",
      " [[246  13]\n",
      " [  9  62]]\n",
      "Dak_Dako SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         dak       0.96      0.95      0.96       259\n",
      "        dako       0.83      0.87      0.85        71\n",
      "\n",
      "    accuracy                           0.93       330\n",
      "   macro avg       0.90      0.91      0.90       330\n",
      "weighted avg       0.94      0.93      0.93       330\n",
      "\n",
      "Bol_Bolo SVM Model Accuracy: 0.9453551912568307\n",
      "Bol_Bolo SVM Confusion Matrix:\n",
      " [[636  21]\n",
      " [ 19  56]]\n",
      "Bol_Bolo SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bol       0.97      0.97      0.97       657\n",
      "        bolo       0.73      0.75      0.74        75\n",
      "\n",
      "    accuracy                           0.95       732\n",
      "   macro avg       0.85      0.86      0.85       732\n",
      "weighted avg       0.95      0.95      0.95       732\n",
      "\n",
      "Kal_Kalo SVM Model Accuracy: 0.9429530201342282\n",
      "Kal_Kalo SVM Confusion Matrix:\n",
      " [[1079   26]\n",
      " [  42   45]]\n",
      "Kal_Kalo SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         kal       0.96      0.98      0.97      1105\n",
      "        kalo       0.63      0.52      0.57        87\n",
      "\n",
      "    accuracy                           0.94      1192\n",
      "   macro avg       0.80      0.75      0.77      1192\n",
      "weighted avg       0.94      0.94      0.94      1192\n",
      "\n",
      "Komol_Komlo SVM Model Accuracy: 0.8738738738738738\n",
      "Komol_Komlo SVM Confusion Matrix:\n",
      " [[72  1]\n",
      " [13 25]]\n",
      "Komol_Komlo SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       komlo       0.85      0.99      0.91        73\n",
      "       komol       0.96      0.66      0.78        38\n",
      "\n",
      "    accuracy                           0.87       111\n",
      "   macro avg       0.90      0.82      0.85       111\n",
      "weighted avg       0.89      0.87      0.87       111\n",
      "\n",
      "Mot_Moto SVM Model Accuracy: 0.9656862745098039\n",
      "Mot_Moto SVM Confusion Matrix:\n",
      " [[392   0]\n",
      " [ 14   2]]\n",
      "Mot_Moto SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         mot       0.97      1.00      0.98       392\n",
      "        moto       1.00      0.12      0.22        16\n",
      "\n",
      "    accuracy                           0.97       408\n",
      "   macro avg       0.98      0.56      0.60       408\n",
      "weighted avg       0.97      0.97      0.95       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate SVM for Dak_Dako\n",
    "model_svm_dak_dako, X_test_dak_dako, y_test_dak_dako = train_evaluate_svm(X_dak_dako, y_dak_dako, 'Dak_Dako')\n",
    "\n",
    "# Train and evaluate SVM for Bol_Bolo\n",
    "model_svm_bol_bolo, X_test_bol_bolo, y_test_bol_bolo = train_evaluate_svm(X_bol_bolo, y_bol_bolo, 'Bol_Bolo')\n",
    "\n",
    "# Train and evaluate SVM for Kal_Kalo\n",
    "model_svm_kal_kalo, X_test_kal_kalo, y_test_kal_kalo = train_evaluate_svm(X_kal_kalo, y_kal_kalo, 'Kal_Kalo')\n",
    "\n",
    "# Train and evaluate SVM for Komol_Komlo\n",
    "model_svm_komol_komlo, X_test_komol_komlo, y_test_komol_komlo = train_evaluate_svm(X_komol_komlo, y_komol_komlo, 'Komol_Komlo')\n",
    "\n",
    "# Train and evaluate SVM for Mot_Moto\n",
    "model_svm_mot_moto, X_test_mot_moto, y_test_mot_moto = train_evaluate_svm(X_mot_moto, y_mot_moto, 'Mot_Moto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train and evaluate a Logistic Regression model\n",
    "def train_evaluate_logistic_regression(X, y, homograph_name):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # Train Logistic Regression model\n",
    "    model_lr = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model_lr.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # Print the evaluation results\n",
    "    print(f\"{homograph_name} Logistic Regression Model Accuracy: {accuracy}\")\n",
    "    print(f\"{homograph_name} Logistic Regression Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(f\"{homograph_name} Logistic Regression Classification Report:\\n\", class_report)\n",
    "    \n",
    "    # Return model and test data for further evaluation if needed\n",
    "    return model_lr, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dak_Dako Logistic Regression Model Accuracy: 0.9212121212121213\n",
      "Dak_Dako Logistic Regression Confusion Matrix:\n",
      " [[244  15]\n",
      " [ 11  60]]\n",
      "Dak_Dako Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         dak       0.96      0.94      0.95       259\n",
      "        dako       0.80      0.85      0.82        71\n",
      "\n",
      "    accuracy                           0.92       330\n",
      "   macro avg       0.88      0.89      0.89       330\n",
      "weighted avg       0.92      0.92      0.92       330\n",
      "\n",
      "Bol_Bolo Logistic Regression Model Accuracy: 0.9508196721311475\n",
      "Bol_Bolo Logistic Regression Confusion Matrix:\n",
      " [[637  20]\n",
      " [ 16  59]]\n",
      "Bol_Bolo Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bol       0.98      0.97      0.97       657\n",
      "        bolo       0.75      0.79      0.77        75\n",
      "\n",
      "    accuracy                           0.95       732\n",
      "   macro avg       0.86      0.88      0.87       732\n",
      "weighted avg       0.95      0.95      0.95       732\n",
      "\n",
      "Kal_Kalo Logistic Regression Model Accuracy: 0.9463087248322147\n",
      "Kal_Kalo Logistic Regression Confusion Matrix:\n",
      " [[1076   29]\n",
      " [  35   52]]\n",
      "Kal_Kalo Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         kal       0.97      0.97      0.97      1105\n",
      "        kalo       0.64      0.60      0.62        87\n",
      "\n",
      "    accuracy                           0.95      1192\n",
      "   macro avg       0.81      0.79      0.80      1192\n",
      "weighted avg       0.94      0.95      0.95      1192\n",
      "\n",
      "Komol_Komlo Logistic Regression Model Accuracy: 0.8558558558558559\n",
      "Komol_Komlo Logistic Regression Confusion Matrix:\n",
      " [[72  1]\n",
      " [15 23]]\n",
      "Komol_Komlo Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       komlo       0.83      0.99      0.90        73\n",
      "       komol       0.96      0.61      0.74        38\n",
      "\n",
      "    accuracy                           0.86       111\n",
      "   macro avg       0.89      0.80      0.82       111\n",
      "weighted avg       0.87      0.86      0.85       111\n",
      "\n",
      "Mot_Moto Logistic Regression Model Accuracy: 0.9705882352941176\n",
      "Mot_Moto Logistic Regression Confusion Matrix:\n",
      " [[392   0]\n",
      " [ 12   4]]\n",
      "Mot_Moto Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         mot       0.97      1.00      0.98       392\n",
      "        moto       1.00      0.25      0.40        16\n",
      "\n",
      "    accuracy                           0.97       408\n",
      "   macro avg       0.99      0.62      0.69       408\n",
      "weighted avg       0.97      0.97      0.96       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Logistic Regression for Dak_Dako\n",
    "model_lr_dak_dako, X_test_dak_dako, y_test_dak_dako = train_evaluate_logistic_regression(X_dak_dako, y_dak_dako, 'Dak_Dako')\n",
    "\n",
    "# Train and evaluate Logistic Regression for Bol_Bolo\n",
    "model_lr_bol_bolo, X_test_bol_bolo, y_test_bol_bolo = train_evaluate_logistic_regression(X_bol_bolo, y_bol_bolo, 'Bol_Bolo')\n",
    "\n",
    "# Train and evaluate Logistic Regression for Kal_Kalo\n",
    "model_lr_kal_kalo, X_test_kal_kalo, y_test_kal_kalo = train_evaluate_logistic_regression(X_kal_kalo, y_kal_kalo, 'Kal_Kalo')\n",
    "\n",
    "# Train and evaluate Logistic Regression for Komol_Komlo\n",
    "model_lr_komol_komlo, X_test_komol_komlo, y_test_komol_komlo = train_evaluate_logistic_regression(X_komol_komlo, y_komol_komlo, 'Komol_Komlo')\n",
    "\n",
    "# Train and evaluate Logistic Regression for Mot_Moto\n",
    "model_lr_mot_moto, X_test_mot_moto, y_test_mot_moto = train_evaluate_logistic_regression(X_mot_moto, y_mot_moto, 'Mot_Moto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train and evaluate a Random Forest model\n",
    "def train_evaluate_random_forest(X, y, homograph_name):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    model_rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model_rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # Print the evaluation results\n",
    "    print(f\"{homograph_name} Random Forest Model Accuracy: {accuracy}\")\n",
    "    print(f\"{homograph_name} Random Forest Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(f\"{homograph_name} Random Forest Classification Report:\\n\", class_report)\n",
    "    \n",
    "    # Return model and test data for further evaluation if needed\n",
    "    return model_rf, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dak_Dako Random Forest Model Accuracy: 0.9333333333333333\n",
      "Dak_Dako Random Forest Confusion Matrix:\n",
      " [[240  19]\n",
      " [  3  68]]\n",
      "Dak_Dako Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         dak       0.99      0.93      0.96       259\n",
      "        dako       0.78      0.96      0.86        71\n",
      "\n",
      "    accuracy                           0.93       330\n",
      "   macro avg       0.88      0.94      0.91       330\n",
      "weighted avg       0.94      0.93      0.94       330\n",
      "\n",
      "Bol_Bolo Random Forest Model Accuracy: 0.9685792349726776\n",
      "Bol_Bolo Random Forest Confusion Matrix:\n",
      " [[648   9]\n",
      " [ 14  61]]\n",
      "Bol_Bolo Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bol       0.98      0.99      0.98       657\n",
      "        bolo       0.87      0.81      0.84        75\n",
      "\n",
      "    accuracy                           0.97       732\n",
      "   macro avg       0.93      0.90      0.91       732\n",
      "weighted avg       0.97      0.97      0.97       732\n",
      "\n",
      "Kal_Kalo Random Forest Model Accuracy: 0.9588926174496645\n",
      "Kal_Kalo Random Forest Confusion Matrix:\n",
      " [[1090   15]\n",
      " [  34   53]]\n",
      "Kal_Kalo Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         kal       0.97      0.99      0.98      1105\n",
      "        kalo       0.78      0.61      0.68        87\n",
      "\n",
      "    accuracy                           0.96      1192\n",
      "   macro avg       0.87      0.80      0.83      1192\n",
      "weighted avg       0.96      0.96      0.96      1192\n",
      "\n",
      "Komol_Komlo Random Forest Model Accuracy: 0.8558558558558559\n",
      "Komol_Komlo Random Forest Confusion Matrix:\n",
      " [[73  0]\n",
      " [16 22]]\n",
      "Komol_Komlo Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       komlo       0.82      1.00      0.90        73\n",
      "       komol       1.00      0.58      0.73        38\n",
      "\n",
      "    accuracy                           0.86       111\n",
      "   macro avg       0.91      0.79      0.82       111\n",
      "weighted avg       0.88      0.86      0.84       111\n",
      "\n",
      "Mot_Moto Random Forest Model Accuracy: 0.9607843137254902\n",
      "Mot_Moto Random Forest Confusion Matrix:\n",
      " [[392   0]\n",
      " [ 16   0]]\n",
      "Mot_Moto Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         mot       0.96      1.00      0.98       392\n",
      "        moto       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.96       408\n",
      "   macro avg       0.48      0.50      0.49       408\n",
      "weighted avg       0.92      0.96      0.94       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Random Forest for Dak_Dako\n",
    "model_rf_dak_dako, X_test_dak_dako, y_test_dak_dako = train_evaluate_random_forest(X_dak_dako, y_dak_dako, 'Dak_Dako')\n",
    "\n",
    "# Train and evaluate Random Forest for Bol_Bolo\n",
    "model_rf_bol_bolo, X_test_bol_bolo, y_test_bol_bolo = train_evaluate_random_forest(X_bol_bolo, y_bol_bolo, 'Bol_Bolo')\n",
    "\n",
    "# Train and evaluate Random Forest for Kal_Kalo\n",
    "model_rf_kal_kalo, X_test_kal_kalo, y_test_kal_kalo = train_evaluate_random_forest(X_kal_kalo, y_kal_kalo, 'Kal_Kalo')\n",
    "\n",
    "# Train and evaluate Random Forest for Komol_Komlo\n",
    "model_rf_komol_komlo, X_test_komol_komlo, y_test_komol_komlo = train_evaluate_random_forest(X_komol_komlo, y_komol_komlo, 'Komol_Komlo')\n",
    "\n",
    "# Train and evaluate Random Forest for Mot_Moto\n",
    "model_rf_mot_moto, X_test_mot_moto, y_test_mot_moto = train_evaluate_random_forest(X_mot_moto, y_mot_moto, 'Mot_Moto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate an XGBoost model without the 'use_label_encoder' parameter\n",
    "def train_evaluate_xgboost(X, y, homograph_name):\n",
    "    # Label encode the target variable\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded)\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    model_xgb = XGBClassifier(eval_metric='mlogloss', random_state=42)  # Removed 'use_label_encoder'\n",
    "    model_xgb.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model_xgb.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # Print the evaluation results\n",
    "    print(f\"{homograph_name} XGBoost Model Accuracy: {accuracy}\")\n",
    "    print(f\"{homograph_name} XGBoost Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(f\"{homograph_name} XGBoost Classification Report:\\n\", class_report)\n",
    "    \n",
    "    # Return model and test data for further evaluation if needed\n",
    "    return model_xgb, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dak_Dako XGBoost Model Accuracy: 0.9393939393939394\n",
      "Dak_Dako XGBoost Confusion Matrix:\n",
      " [[248  11]\n",
      " [  9  62]]\n",
      "Dak_Dako XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       259\n",
      "           1       0.85      0.87      0.86        71\n",
      "\n",
      "    accuracy                           0.94       330\n",
      "   macro avg       0.91      0.92      0.91       330\n",
      "weighted avg       0.94      0.94      0.94       330\n",
      "\n",
      "Bol_Bolo XGBoost Model Accuracy: 0.9549180327868853\n",
      "Bol_Bolo XGBoost Confusion Matrix:\n",
      " [[649   8]\n",
      " [ 25  50]]\n",
      "Bol_Bolo XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       657\n",
      "           1       0.86      0.67      0.75        75\n",
      "\n",
      "    accuracy                           0.95       732\n",
      "   macro avg       0.91      0.83      0.86       732\n",
      "weighted avg       0.95      0.95      0.95       732\n",
      "\n",
      "Kal_Kalo XGBoost Model Accuracy: 0.9521812080536913\n",
      "Kal_Kalo XGBoost Confusion Matrix:\n",
      " [[1103    2]\n",
      " [  55   32]]\n",
      "Kal_Kalo XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      1105\n",
      "           1       0.94      0.37      0.53        87\n",
      "\n",
      "    accuracy                           0.95      1192\n",
      "   macro avg       0.95      0.68      0.75      1192\n",
      "weighted avg       0.95      0.95      0.94      1192\n",
      "\n",
      "Komol_Komlo XGBoost Model Accuracy: 0.8018018018018018\n",
      "Komol_Komlo XGBoost Confusion Matrix:\n",
      " [[66  7]\n",
      " [15 23]]\n",
      "Komol_Komlo XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.86        73\n",
      "           1       0.77      0.61      0.68        38\n",
      "\n",
      "    accuracy                           0.80       111\n",
      "   macro avg       0.79      0.75      0.77       111\n",
      "weighted avg       0.80      0.80      0.80       111\n",
      "\n",
      "Mot_Moto XGBoost Model Accuracy: 0.9632352941176471\n",
      "Mot_Moto XGBoost Confusion Matrix:\n",
      " [[392   0]\n",
      " [ 15   1]]\n",
      "Mot_Moto XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       392\n",
      "           1       1.00      0.06      0.12        16\n",
      "\n",
      "    accuracy                           0.96       408\n",
      "   macro avg       0.98      0.53      0.55       408\n",
      "weighted avg       0.96      0.96      0.95       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate XGBoost for Dak_Dako\n",
    "model_xgb_dak_dako, X_test_dak_dako, y_test_dak_dako = train_evaluate_xgboost(X_dak_dako, y_dak_dako, 'Dak_Dako')\n",
    "\n",
    "# Train and evaluate XGBoost for Bol_Bolo\n",
    "model_xgb_bol_bolo, X_test_bol_bolo, y_test_bol_bolo = train_evaluate_xgboost(X_bol_bolo, y_bol_bolo, 'Bol_Bolo')\n",
    "\n",
    "# Train and evaluate XGBoost for Kal_Kalo\n",
    "model_xgb_kal_kalo, X_test_kal_kalo, y_test_kal_kalo = train_evaluate_xgboost(X_kal_kalo, y_kal_kalo, 'Kal_Kalo')\n",
    "\n",
    "# Train and evaluate XGBoost for Komol_Komlo\n",
    "model_xgb_komol_komlo, X_test_komol_komlo, y_test_komol_komlo = train_evaluate_xgboost(X_komol_komlo, y_komol_komlo, 'Komol_Komlo')\n",
    "\n",
    "# Train and evaluate XGBoost for Mot_Moto\n",
    "model_xgb_mot_moto, X_test_mot_moto, y_test_mot_moto = train_evaluate_xgboost(X_mot_moto, y_mot_moto, 'Mot_Moto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Function for Hyperparameter Tuning of SVM\n",
    "def hyperparameter_tuning_svm(X, y):\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "    grid_search = GridSearchCV(SVC(class_weight='balanced', random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    print(f\"Best Hyperparameters for SVM: {grid_search.best_params_}\")\n",
    "    print(f\"Best SVM Model Accuracy: {grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Function for Hyperparameter Tuning of Logistic Regression\n",
    "def hyperparameter_tuning_logistic_regression(X, y):\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    }\n",
    "    grid_search = GridSearchCV(LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000), param_grid, cv=3, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    print(f\"Best Hyperparameters for Logistic Regression: {grid_search.best_params_}\")\n",
    "    print(f\"Best Logistic Regression Model Accuracy: {grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Function for Hyperparameter Tuning of Random Forest\n",
    "def hyperparameter_tuning_random_forest(X, y):\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "    }\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    print(f\"Best Hyperparameters for Random Forest: {grid_search.best_params_}\")\n",
    "    print(f\"Best Random Forest Model Accuracy: {grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Function for Hyperparameter Tuning of XGBoost\n",
    "def hyperparameter_tuning_xgboost(X, y):\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    grid_search = GridSearchCV(XGBClassifier(eval_metric='mlogloss', random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    print(f\"Best Hyperparameters for XGBoost: {grid_search.best_params_}\")\n",
    "    print(f\"Best XGBoost Model Accuracy: {grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function for Hyperparameter Tuning of XGBoost with Label Encoding\n",
    "def hyperparameter_tuning_xgboost(X, y):\n",
    "    # Encode labels as integers\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    grid_search = GridSearchCV(XGBClassifier(eval_metric='mlogloss', random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "    grid_search.fit(X, y_encoded)\n",
    "    \n",
    "    print(f\"Best Hyperparameters for XGBoost: {grid_search.best_params_}\")\n",
    "    print(f\"Best XGBoost Model Accuracy: {grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning for Dak_Dako\n",
      "Best Hyperparameters for SVM: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best SVM Model Accuracy: 0.9434862888938792\n",
      "Best Hyperparameters for Logistic Regression: {'C': 10, 'solver': 'newton-cg'}\n",
      "Best Logistic Regression Model Accuracy: 0.9033385732465006\n",
      "Best Hyperparameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best Random Forest Model Accuracy: 0.9133792449534646\n",
      "Best Hyperparameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Best XGBoost Model Accuracy: 0.9170421937769793\n",
      "\n",
      "Hyperparameter Tuning for Bol_Bolo\n",
      "Best Hyperparameters for SVM: {'C': 100, 'gamma': 1, 'kernel': 'linear'}\n",
      "Best SVM Model Accuracy: 0.951209512095121\n",
      "Best Hyperparameters for Logistic Regression: {'C': 100, 'solver': 'newton-cg'}\n",
      "Best Logistic Regression Model Accuracy: 0.9553095530955309\n",
      "Best Hyperparameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Best Random Forest Model Accuracy: 0.957359573595736\n",
      "Best Hyperparameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Best XGBoost Model Accuracy: 0.9483394833948339\n",
      "\n",
      "Hyperparameter Tuning for Kal_Kalo\n",
      "Best Hyperparameters for SVM: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best SVM Model Accuracy: 0.9614803625377643\n",
      "Best Hyperparameters for Logistic Regression: {'C': 10, 'solver': 'newton-cg'}\n",
      "Best Logistic Regression Model Accuracy: 0.964501510574018\n",
      "Best Hyperparameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best Random Forest Model Accuracy: 0.9592145015105741\n",
      "Best Hyperparameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Best XGBoost Model Accuracy: 0.9549345417925478\n",
      "\n",
      "Hyperparameter Tuning for Komol_Komlo\n",
      "Best Hyperparameters for SVM: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best SVM Model Accuracy: 0.9110280618935221\n",
      "Best Hyperparameters for Logistic Regression: {'C': 10, 'solver': 'newton-cg'}\n",
      "Best Logistic Regression Model Accuracy: 0.9191581431943351\n",
      "Best Hyperparameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Random Forest Model Accuracy: 0.8840589212343737\n",
      "Best Hyperparameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Best XGBoost Model Accuracy: 0.8435396450738701\n",
      "\n",
      "Hyperparameter Tuning for Mot_Moto\n",
      "Best Hyperparameters for SVM: {'C': 1, 'gamma': 1, 'kernel': 'linear'}\n",
      "Best SVM Model Accuracy: 0.9639180292640998\n",
      "Best Hyperparameters for Logistic Regression: {'C': 10, 'solver': 'newton-cg'}\n",
      "Best Logistic Regression Model Accuracy: 0.9639196572180221\n",
      "Best Hyperparameters for Random Forest: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Best Random Forest Model Accuracy: 0.9639180292640998\n",
      "Best Hyperparameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Best XGBoost Model Accuracy: 0.9624447309643348\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have preprocessed and vectorized data for all homographs:\n",
    "# Dak_Dako: X_dak_dako, y_dak_dako\n",
    "# Bol_Bolo: X_bol_bolo, y_bol_bolo\n",
    "# Kal_Kalo: X_kal_kalo, y_kal_kalo\n",
    "# Komol_Komlo: X_komol_komlo, y_komol_komlo\n",
    "# Mot_Moto: X_mot_moto, y_mot_moto\n",
    "\n",
    "# Hyperparameter Tuning for Dak_Dako\n",
    "print(\"Hyperparameter Tuning for Dak_Dako\")\n",
    "best_svm_dak_dako = hyperparameter_tuning_svm(X_dak_dako, y_dak_dako)\n",
    "best_lr_dak_dako = hyperparameter_tuning_logistic_regression(X_dak_dako, y_dak_dako)\n",
    "best_rf_dak_dako = hyperparameter_tuning_random_forest(X_dak_dako, y_dak_dako)\n",
    "best_xgb_dak_dako = hyperparameter_tuning_xgboost(X_dak_dako, y_dak_dako)  # Updated function with Label Encoding\n",
    "\n",
    "# Hyperparameter Tuning for Bol_Bolo\n",
    "print(\"\\nHyperparameter Tuning for Bol_Bolo\")\n",
    "best_svm_bol_bolo = hyperparameter_tuning_svm(X_bol_bolo, y_bol_bolo)\n",
    "best_lr_bol_bolo = hyperparameter_tuning_logistic_regression(X_bol_bolo, y_bol_bolo)\n",
    "best_rf_bol_bolo = hyperparameter_tuning_random_forest(X_bol_bolo, y_bol_bolo)\n",
    "best_xgb_bol_bolo = hyperparameter_tuning_xgboost(X_bol_bolo, y_bol_bolo)  # Updated function with Label Encoding\n",
    "\n",
    "# Hyperparameter Tuning for Kal_Kalo\n",
    "print(\"\\nHyperparameter Tuning for Kal_Kalo\")\n",
    "best_svm_kal_kalo = hyperparameter_tuning_svm(X_kal_kalo, y_kal_kalo)\n",
    "best_lr_kal_kalo = hyperparameter_tuning_logistic_regression(X_kal_kalo, y_kal_kalo)\n",
    "best_rf_kal_kalo = hyperparameter_tuning_random_forest(X_kal_kalo, y_kal_kalo)\n",
    "best_xgb_kal_kalo = hyperparameter_tuning_xgboost(X_kal_kalo, y_kal_kalo)  # Updated function with Label Encoding\n",
    "\n",
    "# Hyperparameter Tuning for Komol_Komlo\n",
    "print(\"\\nHyperparameter Tuning for Komol_Komlo\")\n",
    "best_svm_komol_komlo = hyperparameter_tuning_svm(X_komol_komlo, y_komol_komlo)\n",
    "best_lr_komol_komlo = hyperparameter_tuning_logistic_regression(X_komol_komlo, y_komol_komlo)\n",
    "best_rf_komol_komlo = hyperparameter_tuning_random_forest(X_komol_komlo, y_komol_komlo)\n",
    "best_xgb_komol_komlo = hyperparameter_tuning_xgboost(X_komol_komlo, y_komol_komlo)  # Updated function with Label Encoding\n",
    "\n",
    "# Hyperparameter Tuning for Mot_Moto\n",
    "print(\"\\nHyperparameter Tuning for Mot_Moto\")\n",
    "best_svm_mot_moto = hyperparameter_tuning_svm(X_mot_moto, y_mot_moto)\n",
    "best_lr_mot_moto = hyperparameter_tuning_logistic_regression(X_mot_moto, y_mot_moto)\n",
    "best_rf_mot_moto = hyperparameter_tuning_random_forest(X_mot_moto, y_mot_moto)\n",
    "best_xgb_mot_moto = hyperparameter_tuning_xgboost(X_mot_moto, y_mot_moto)  # Updated function with Label Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning for Naive Bayes - Dak_Dako\n",
      "Best Hyperparameters for Naive Bayes: {'alpha': 0.01}\n",
      "Best Naive Bayes Model Accuracy: 0.8623474810988846\n",
      "\n",
      "Hyperparameter Tuning for Naive Bayes - Bol_Bolo\n",
      "Best Hyperparameters for Naive Bayes: {'alpha': 0.01}\n",
      "Best Naive Bayes Model Accuracy: 0.9430094300943009\n",
      "\n",
      "Hyperparameter Tuning for Naive Bayes - Kal_Kalo\n",
      "Best Hyperparameters for Naive Bayes: {'alpha': 0.01}\n",
      "Best Naive Bayes Model Accuracy: 0.951409869083585\n",
      "\n",
      "Hyperparameter Tuning for Naive Bayes - Komol_Komlo\n",
      "Best Hyperparameters for Naive Bayes: {'alpha': 0.5}\n",
      "Best Naive Bayes Model Accuracy: 0.9190707229652942\n",
      "\n",
      "Hyperparameter Tuning for Naive Bayes - Mot_Moto\n",
      "Best Hyperparameters for Naive Bayes: {'alpha': 0.5}\n",
      "Best Naive Bayes Model Accuracy: 0.9609730606184922\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Function for Hyperparameter Tuning of Naive Bayes\n",
    "def hyperparameter_tuning_naive_bayes(X, y):\n",
    "    # Define hyperparameter grid for Naive Bayes\n",
    "    param_grid = {\n",
    "        'alpha': [0.01, 0.1, 0.5, 1, 2, 5, 10]  # Smoothing parameter\n",
    "    }\n",
    "    # Create GridSearchCV with MultinomialNB\n",
    "    grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=3, scoring='accuracy')\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Print the best hyperparameters and the best score\n",
    "    print(f\"Best Hyperparameters for Naive Bayes: {grid_search.best_params_}\")\n",
    "    print(f\"Best Naive Bayes Model Accuracy: {grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Hyperparameter Tuning for Dak_Dako\n",
    "print(\"Hyperparameter Tuning for Naive Bayes - Dak_Dako\")\n",
    "best_naive_bayes_dak_dako = hyperparameter_tuning_naive_bayes(X_dak_dako, y_dak_dako)\n",
    "\n",
    "# Hyperparameter Tuning for Bol_Bolo\n",
    "print(\"\\nHyperparameter Tuning for Naive Bayes - Bol_Bolo\")\n",
    "best_naive_bayes_bol_bolo = hyperparameter_tuning_naive_bayes(X_bol_bolo, y_bol_bolo)\n",
    "\n",
    "# Hyperparameter Tuning for Kal_Kalo\n",
    "print(\"\\nHyperparameter Tuning for Naive Bayes - Kal_Kalo\")\n",
    "best_naive_bayes_kal_kalo = hyperparameter_tuning_naive_bayes(X_kal_kalo, y_kal_kalo)\n",
    "\n",
    "# Hyperparameter Tuning for Komol_Komlo\n",
    "print(\"\\nHyperparameter Tuning for Naive Bayes - Komol_Komlo\")\n",
    "best_naive_bayes_komol_komlo = hyperparameter_tuning_naive_bayes(X_komol_komlo, y_komol_komlo)\n",
    "\n",
    "# Hyperparameter Tuning for Mot_Moto\n",
    "print(\"\\nHyperparameter Tuning for Naive Bayes - Mot_Moto\")\n",
    "best_naive_bayes_mot_moto = hyperparameter_tuning_naive_bayes(X_mot_moto, y_mot_moto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dak_Dako Naive Bayes Performance ---\n",
      "Accuracy: 0.8212121212121212\n",
      "Confusion Matrix:\n",
      "[[259   0]\n",
      " [ 59  12]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90       259\n",
      "           1       1.00      0.17      0.29        71\n",
      "\n",
      "    accuracy                           0.82       330\n",
      "   macro avg       0.91      0.58      0.59       330\n",
      "weighted avg       0.85      0.82      0.77       330\n",
      "\n",
      "\n",
      "--- Dak_Dako SVM Performance ---\n",
      "Accuracy: 0.9212121212121213\n",
      "Confusion Matrix:\n",
      "[[257   2]\n",
      " [ 24  47]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95       259\n",
      "           1       0.96      0.66      0.78        71\n",
      "\n",
      "    accuracy                           0.92       330\n",
      "   macro avg       0.94      0.83      0.87       330\n",
      "weighted avg       0.92      0.92      0.92       330\n",
      "\n",
      "\n",
      "--- Dak_Dako Logistic Regression Performance ---\n",
      "Accuracy: 0.8515151515151516\n",
      "Confusion Matrix:\n",
      "[[258   1]\n",
      " [ 48  23]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       259\n",
      "           1       0.96      0.32      0.48        71\n",
      "\n",
      "    accuracy                           0.85       330\n",
      "   macro avg       0.90      0.66      0.70       330\n",
      "weighted avg       0.87      0.85      0.82       330\n",
      "\n",
      "\n",
      "--- Dak_Dako Random Forest Performance ---\n",
      "Accuracy: 0.9272727272727272\n",
      "Confusion Matrix:\n",
      "[[242  17]\n",
      " [  7  64]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       259\n",
      "           1       0.79      0.90      0.84        71\n",
      "\n",
      "    accuracy                           0.93       330\n",
      "   macro avg       0.88      0.92      0.90       330\n",
      "weighted avg       0.93      0.93      0.93       330\n",
      "\n",
      "\n",
      "--- Dak_Dako XGBoost Performance ---\n",
      "Accuracy: 0.9363636363636364\n",
      "Confusion Matrix:\n",
      "[[246  13]\n",
      " [  8  63]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       259\n",
      "           1       0.83      0.89      0.86        71\n",
      "\n",
      "    accuracy                           0.94       330\n",
      "   macro avg       0.90      0.92      0.91       330\n",
      "weighted avg       0.94      0.94      0.94       330\n",
      "\n",
      "\n",
      "--- Bol_Bolo Naive Bayes Performance ---\n",
      "Accuracy: 0.912568306010929\n",
      "Confusion Matrix:\n",
      "[[657   0]\n",
      " [ 64  11]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95       657\n",
      "           1       1.00      0.15      0.26        75\n",
      "\n",
      "    accuracy                           0.91       732\n",
      "   macro avg       0.96      0.57      0.60       732\n",
      "weighted avg       0.92      0.91      0.88       732\n",
      "\n",
      "\n",
      "--- Bol_Bolo SVM Performance ---\n",
      "Accuracy: 0.9521857923497268\n",
      "Confusion Matrix:\n",
      "[[652   5]\n",
      " [ 30  45]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       657\n",
      "           1       0.90      0.60      0.72        75\n",
      "\n",
      "    accuracy                           0.95       732\n",
      "   macro avg       0.93      0.80      0.85       732\n",
      "weighted avg       0.95      0.95      0.95       732\n",
      "\n",
      "\n",
      "--- Bol_Bolo Logistic Regression Performance ---\n",
      "Accuracy: 0.9153005464480874\n",
      "Confusion Matrix:\n",
      "[[653   4]\n",
      " [ 58  17]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95       657\n",
      "           1       0.81      0.23      0.35        75\n",
      "\n",
      "    accuracy                           0.92       732\n",
      "   macro avg       0.86      0.61      0.65       732\n",
      "weighted avg       0.91      0.92      0.89       732\n",
      "\n",
      "\n",
      "--- Bol_Bolo Random Forest Performance ---\n",
      "Accuracy: 0.9672131147540983\n",
      "Confusion Matrix:\n",
      "[[651   6]\n",
      " [ 18  57]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       657\n",
      "           1       0.90      0.76      0.83        75\n",
      "\n",
      "    accuracy                           0.97       732\n",
      "   macro avg       0.94      0.88      0.90       732\n",
      "weighted avg       0.97      0.97      0.97       732\n",
      "\n",
      "\n",
      "--- Bol_Bolo XGBoost Performance ---\n",
      "Accuracy: 0.9494535519125683\n",
      "Confusion Matrix:\n",
      "[[648   9]\n",
      " [ 28  47]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       657\n",
      "           1       0.84      0.63      0.72        75\n",
      "\n",
      "    accuracy                           0.95       732\n",
      "   macro avg       0.90      0.81      0.84       732\n",
      "weighted avg       0.95      0.95      0.95       732\n",
      "\n",
      "\n",
      "--- Kal_Kalo Naive Bayes Performance ---\n",
      "Accuracy: 0.9312080536912751\n",
      "Confusion Matrix:\n",
      "[[1105    0]\n",
      " [  82    5]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1105\n",
      "           1       1.00      0.06      0.11        87\n",
      "\n",
      "    accuracy                           0.93      1192\n",
      "   macro avg       0.97      0.53      0.54      1192\n",
      "weighted avg       0.94      0.93      0.90      1192\n",
      "\n",
      "\n",
      "--- Kal_Kalo SVM Performance ---\n",
      "Accuracy: 0.9555369127516778\n",
      "Confusion Matrix:\n",
      "[[1105    0]\n",
      " [  53   34]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      1105\n",
      "           1       1.00      0.39      0.56        87\n",
      "\n",
      "    accuracy                           0.96      1192\n",
      "   macro avg       0.98      0.70      0.77      1192\n",
      "weighted avg       0.96      0.96      0.95      1192\n",
      "\n",
      "\n",
      "--- Kal_Kalo Logistic Regression Performance ---\n",
      "Accuracy: 0.9395973154362416\n",
      "Confusion Matrix:\n",
      "[[1105    0]\n",
      " [  72   15]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      1105\n",
      "           1       1.00      0.17      0.29        87\n",
      "\n",
      "    accuracy                           0.94      1192\n",
      "   macro avg       0.97      0.59      0.63      1192\n",
      "weighted avg       0.94      0.94      0.92      1192\n",
      "\n",
      "\n",
      "--- Kal_Kalo Random Forest Performance ---\n",
      "Accuracy: 0.9630872483221476\n",
      "Confusion Matrix:\n",
      "[[1104    1]\n",
      " [  43   44]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1105\n",
      "           1       0.98      0.51      0.67        87\n",
      "\n",
      "    accuracy                           0.96      1192\n",
      "   macro avg       0.97      0.75      0.82      1192\n",
      "weighted avg       0.96      0.96      0.96      1192\n",
      "\n",
      "\n",
      "--- Kal_Kalo XGBoost Performance ---\n",
      "Accuracy: 0.9521812080536913\n",
      "Confusion Matrix:\n",
      "[[1102    3]\n",
      " [  54   33]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      1105\n",
      "           1       0.92      0.38      0.54        87\n",
      "\n",
      "    accuracy                           0.95      1192\n",
      "   macro avg       0.93      0.69      0.76      1192\n",
      "weighted avg       0.95      0.95      0.94      1192\n",
      "\n",
      "\n",
      "--- Komol_Komlo Naive Bayes Performance ---\n",
      "Accuracy: 0.8468468468468469\n",
      "Confusion Matrix:\n",
      "[[73  0]\n",
      " [17 21]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90        73\n",
      "           1       1.00      0.55      0.71        38\n",
      "\n",
      "    accuracy                           0.85       111\n",
      "   macro avg       0.91      0.78      0.80       111\n",
      "weighted avg       0.88      0.85      0.83       111\n",
      "\n",
      "\n",
      "--- Komol_Komlo SVM Performance ---\n",
      "Accuracy: 0.7927927927927928\n",
      "Confusion Matrix:\n",
      "[[73  0]\n",
      " [23 15]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86        73\n",
      "           1       1.00      0.39      0.57        38\n",
      "\n",
      "    accuracy                           0.79       111\n",
      "   macro avg       0.88      0.70      0.71       111\n",
      "weighted avg       0.84      0.79      0.76       111\n",
      "\n",
      "\n",
      "--- Komol_Komlo Logistic Regression Performance ---\n",
      "Accuracy: 0.7747747747747747\n",
      "Confusion Matrix:\n",
      "[[73  0]\n",
      " [25 13]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85        73\n",
      "           1       1.00      0.34      0.51        38\n",
      "\n",
      "    accuracy                           0.77       111\n",
      "   macro avg       0.87      0.67      0.68       111\n",
      "weighted avg       0.83      0.77      0.74       111\n",
      "\n",
      "\n",
      "--- Komol_Komlo Random Forest Performance ---\n",
      "Accuracy: 0.8198198198198198\n",
      "Confusion Matrix:\n",
      "[[73  0]\n",
      " [20 18]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88        73\n",
      "           1       1.00      0.47      0.64        38\n",
      "\n",
      "    accuracy                           0.82       111\n",
      "   macro avg       0.89      0.74      0.76       111\n",
      "weighted avg       0.86      0.82      0.80       111\n",
      "\n",
      "\n",
      "--- Komol_Komlo XGBoost Performance ---\n",
      "Accuracy: 0.8288288288288288\n",
      "Confusion Matrix:\n",
      "[[70  3]\n",
      " [16 22]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.96      0.88        73\n",
      "           1       0.88      0.58      0.70        38\n",
      "\n",
      "    accuracy                           0.83       111\n",
      "   macro avg       0.85      0.77      0.79       111\n",
      "weighted avg       0.84      0.83      0.82       111\n",
      "\n",
      "\n",
      "--- Mot_Moto Naive Bayes Performance ---\n",
      "Accuracy: 0.9607843137254902\n",
      "Confusion Matrix:\n",
      "[[392   0]\n",
      " [ 16   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       392\n",
      "           1       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.96       408\n",
      "   macro avg       0.48      0.50      0.49       408\n",
      "weighted avg       0.92      0.96      0.94       408\n",
      "\n",
      "\n",
      "--- Mot_Moto SVM Performance ---\n",
      "Accuracy: 0.9607843137254902\n",
      "Confusion Matrix:\n",
      "[[392   0]\n",
      " [ 16   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       392\n",
      "           1       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.96       408\n",
      "   macro avg       0.48      0.50      0.49       408\n",
      "weighted avg       0.92      0.96      0.94       408\n",
      "\n",
      "\n",
      "--- Mot_Moto Logistic Regression Performance ---\n",
      "Accuracy: 0.9607843137254902\n",
      "Confusion Matrix:\n",
      "[[392   0]\n",
      " [ 16   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       392\n",
      "           1       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.96       408\n",
      "   macro avg       0.48      0.50      0.49       408\n",
      "weighted avg       0.92      0.96      0.94       408\n",
      "\n",
      "\n",
      "--- Mot_Moto Random Forest Performance ---\n",
      "Accuracy: 0.9607843137254902\n",
      "Confusion Matrix:\n",
      "[[392   0]\n",
      " [ 16   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       392\n",
      "           1       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.96       408\n",
      "   macro avg       0.48      0.50      0.49       408\n",
      "weighted avg       0.92      0.96      0.94       408\n",
      "\n",
      "\n",
      "--- Mot_Moto XGBoost Performance ---\n",
      "Accuracy: 0.9607843137254902\n",
      "Confusion Matrix:\n",
      "[[390   2]\n",
      " [ 14   2]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       392\n",
      "           1       0.50      0.12      0.20        16\n",
      "\n",
      "    accuracy                           0.96       408\n",
      "   macro avg       0.73      0.56      0.59       408\n",
      "weighted avg       0.95      0.96      0.95       408\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppress specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "# Import necessary libraries\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Function to read and preprocess text files\n",
    "def read_and_preprocess(file_path, label):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    # Remove extra spaces and add a label\n",
    "    data = [(line.strip(), label) for line in lines if line.strip()]\n",
    "    return data\n",
    "\n",
    "# Function to combine multiple labeled datasets into a single DataFrame\n",
    "def combine_data(*data_sets):\n",
    "    combined_data = []\n",
    "    for data in data_sets:\n",
    "        combined_data.extend(data)\n",
    "    return pd.DataFrame(combined_data, columns=['text', 'label'])\n",
    "\n",
    "# Function to vectorize text data using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize_text(data_frame):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))  # Using n-grams up to 3\n",
    "    X = vectorizer.fit_transform(data_frame['text'])\n",
    "    return X, data_frame['label'], vectorizer\n",
    "\n",
    "# Function to evaluate the model with zero_division handling\n",
    "def evaluate_model_zero_division(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "    return accuracy, conf_matrix, class_report\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_evaluate_model(X, y, model, model_name, homograph_name):\n",
    "    # Encode the labels before training\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    print(f\"--- {homograph_name} {model_name} Performance ---\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(f\"Classification Report:\\n{class_report}\\n\")\n",
    "\n",
    "# File paths for each homograph pair\n",
    "file_paths_dak_dako = [\n",
    "    (\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\dak_dako\\\\dak.txt\", 'dak'),\n",
    "    (\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\dak_dako\\\\dako.txt\", 'dako')\n",
    "]\n",
    "file_paths_bol_bolo = [\n",
    "    (\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\bol_bolo\\\\Bol.txt\", 'bol'),\n",
    "    (\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\bol_bolo\\\\Bolo.txt\", 'bolo')\n",
    "]\n",
    "file_paths_kal_kalo = [\n",
    "    (\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\kal_kalo\\\\Kal.txt\", 'kal'),\n",
    "    (\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\kal_kalo\\\\kalo.txt\", 'kalo')\n",
    "]\n",
    "file_paths_komol_komlo = [\n",
    "    (\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\komol_komlo\\\\komol.txt\", 'komol'),\n",
    "    (\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\komol_komlo\\\\komlo.txt\", 'komlo')\n",
    "]\n",
    "file_paths_mot_moto = [\n",
    "    (\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\mot_moto\\\\mot.txt\", 'mot'),\n",
    "    (\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\mot_moto\\\\moto.txt\", 'moto')\n",
    "]\n",
    "\n",
    "# Function to process and evaluate each homograph dataset with multiple models\n",
    "def process_and_evaluate_homograph(homograph_name, file_paths):\n",
    "    # Read and preprocess data for each file\n",
    "    data = []\n",
    "    for file_path, label in file_paths:\n",
    "        data.extend(read_and_preprocess(file_path, label))\n",
    "    \n",
    "    # Combine and vectorize the text data\n",
    "    df_homograph = combine_data(data)\n",
    "    X_homograph, y_homograph, vectorizer_homograph = vectorize_text(df_homograph)\n",
    "    \n",
    "    # Define models to evaluate\n",
    "    models = {\n",
    "        'Naive Bayes': MultinomialNB(),\n",
    "        'SVM': SVC(probability=True),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=2000),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(eval_metric='mlogloss', random_state=42, use_label_encoder=False)\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        train_evaluate_model(X_homograph, y_homograph, model, model_name, homograph_name)\n",
    "\n",
    "# Process and evaluate each homograph dataset\n",
    "process_and_evaluate_homograph(\"Dak_Dako\", file_paths_dak_dako)\n",
    "process_and_evaluate_homograph(\"Bol_Bolo\", file_paths_bol_bolo)\n",
    "process_and_evaluate_homograph(\"Kal_Kalo\", file_paths_kal_kalo)\n",
    "process_and_evaluate_homograph(\"Komol_Komlo\", file_paths_komol_komlo)\n",
    "process_and_evaluate_homograph(\"Mot_Moto\", file_paths_mot_moto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     -------------------------- ----------- 30.7/43.7 kB 660.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.7/43.7 kB 711.0 kB/s eta 0:00:00\n",
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp310-cp310-win_amd64.whl.metadata (27 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kamru\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kamru\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kamru\\.conda\\envs\\py310\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.6/9.5 MB 19.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.4/9.5 MB 18.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.2/9.5 MB 20.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.0/9.5 MB 21.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.8/9.5 MB 22.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.6/9.5 MB 20.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.4/9.5 MB 21.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.2/9.5 MB 21.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.0/9.5 MB 21.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.5/9.5 MB 20.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.4/9.5 MB 21.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.2/9.5 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 20.9 MB/s eta 0:00:00\n",
      "Downloading torch-2.4.1-cp310-cp310-win_amd64.whl (199.4 MB)\n",
      "   ---------------------------------------- 0.0/199.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/199.4 MB 16.8 MB/s eta 0:00:12\n",
      "   ---------------------------------------- 1.6/199.4 MB 20.1 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 2.4/199.4 MB 21.6 MB/s eta 0:00:10\n",
      "    --------------------------------------- 3.2/199.4 MB 22.6 MB/s eta 0:00:09\n",
      "    --------------------------------------- 4.0/199.4 MB 21.4 MB/s eta 0:00:10\n",
      "    --------------------------------------- 4.8/199.4 MB 22.0 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 5.6/199.4 MB 22.4 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 6.4/199.4 MB 21.5 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 7.2/199.4 MB 21.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 8.0/199.4 MB 22.2 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 8.8/199.4 MB 22.4 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 9.6/199.4 MB 21.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 10.3/199.4 MB 22.6 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 11.1/199.4 MB 22.6 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 11.9/199.4 MB 22.6 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 12.7/199.4 MB 21.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 13.5/199.4 MB 22.6 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 14.3/199.4 MB 22.6 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 15.1/199.4 MB 22.6 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 16.0/199.4 MB 22.6 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 16.8/199.4 MB 22.6 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 17.5/199.4 MB 21.8 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 18.3/199.4 MB 21.8 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 19.1/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 19.9/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 20.7/199.4 MB 21.8 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 21.5/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 22.3/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 23.1/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 23.8/199.4 MB 21.8 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 24.6/199.4 MB 21.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 25.4/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 26.2/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 27.0/199.4 MB 21.8 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 27.8/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 28.6/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 29.4/199.4 MB 21.8 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 30.2/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 31.0/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 31.8/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 32.6/199.4 MB 21.8 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 33.4/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 34.2/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 35.0/199.4 MB 22.5 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 35.7/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 36.5/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 37.4/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 38.2/199.4 MB 21.8 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 38.4/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 39.7/199.4 MB 22.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 40.5/199.4 MB 22.5 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 41.3/199.4 MB 21.9 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 42.1/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 42.9/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 43.7/199.4 MB 21.8 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 44.5/199.4 MB 21.8 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 45.2/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 46.0/199.4 MB 22.5 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 46.8/199.4 MB 21.9 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 47.6/199.4 MB 21.8 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 48.4/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 49.2/199.4 MB 23.4 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 50.0/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 50.8/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 51.6/199.4 MB 22.5 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 52.4/199.4 MB 21.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 53.2/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 54.0/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 54.8/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 55.6/199.4 MB 21.8 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 56.3/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 57.1/199.4 MB 22.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 58.0/199.4 MB 21.9 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 58.8/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 59.5/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 60.3/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 61.1/199.4 MB 21.8 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 61.9/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 62.7/199.4 MB 22.5 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 63.5/199.4 MB 22.6 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 64.3/199.4 MB 21.8 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 65.1/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 65.9/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 66.7/199.4 MB 21.8 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 67.5/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 68.2/199.4 MB 22.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 69.0/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 69.8/199.4 MB 21.8 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 70.7/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 71.5/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 72.3/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 73.1/199.4 MB 21.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 73.8/199.4 MB 22.5 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 74.6/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 75.4/199.4 MB 21.8 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 76.2/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 77.0/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 77.8/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 78.6/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 79.5/199.4 MB 22.5 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 80.2/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 81.0/199.4 MB 21.8 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 81.8/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 82.6/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 83.4/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 84.2/199.4 MB 21.9 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 84.9/199.4 MB 22.5 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 85.8/199.4 MB 22.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 86.7/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 87.5/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 88.2/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 89.1/199.4 MB 21.8 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 89.9/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 90.7/199.4 MB 22.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 91.5/199.4 MB 21.9 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 92.3/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 93.1/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 93.8/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 94.6/199.4 MB 21.8 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 95.4/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 96.2/199.4 MB 22.5 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 97.0/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 97.8/199.4 MB 21.8 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 98.6/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 99.4/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 100.1/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 100.9/199.4 MB 21.9 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 101.7/199.4 MB 22.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 102.6/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 103.3/199.4 MB 21.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 104.2/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 104.9/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 105.8/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 106.5/199.4 MB 21.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 107.3/199.4 MB 22.5 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 108.1/199.4 MB 22.6 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 108.9/199.4 MB 21.8 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 109.0/199.4 MB 21.8 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 110.4/199.4 MB 22.6 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 111.2/199.4 MB 22.6 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 112.0/199.4 MB 21.9 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 112.8/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 113.6/199.4 MB 22.6 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 114.5/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 115.2/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 116.0/199.4 MB 22.6 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 116.8/199.4 MB 22.6 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 117.6/199.4 MB 21.9 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 118.4/199.4 MB 22.5 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 119.2/199.4 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 120.0/199.4 MB 22.6 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 120.8/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 121.6/199.4 MB 22.6 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 122.4/199.4 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 123.2/199.4 MB 21.9 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 123.7/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 124.6/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 125.4/199.4 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 126.2/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 126.9/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 127.7/199.4 MB 21.9 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 128.5/199.4 MB 21.9 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 129.3/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 130.1/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 130.9/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 131.8/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 132.5/199.4 MB 21.8 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 133.1/199.4 MB 21.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 134.1/199.4 MB 21.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 134.9/199.4 MB 21.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 135.7/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 136.5/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 137.3/199.4 MB 21.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 138.1/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 138.9/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 139.7/199.4 MB 21.9 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 140.5/199.4 MB 22.5 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 141.3/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 142.1/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 142.9/199.4 MB 21.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 143.7/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 144.5/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 145.2/199.4 MB 21.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 146.0/199.4 MB 21.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 146.8/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 147.7/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 148.5/199.4 MB 21.8 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 149.2/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 150.0/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 150.8/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 151.6/199.4 MB 21.8 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 152.4/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 153.2/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 153.9/199.4 MB 22.6 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 154.7/199.4 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 155.5/199.4 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 156.3/199.4 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 157.1/199.4 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 157.9/199.4 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 158.7/199.4 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 159.5/199.4 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 160.2/199.4 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 161.0/199.4 MB 21.9 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 161.8/199.4 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 162.4/199.4 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 163.0/199.4 MB 21.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 164.0/199.4 MB 21.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 164.7/199.4 MB 21.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 165.4/199.4 MB 21.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 166.1/199.4 MB 21.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 166.7/199.4 MB 21.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 167.5/199.4 MB 21.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 168.2/199.4 MB 20.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 168.9/199.4 MB 20.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 169.6/199.4 MB 20.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 170.3/199.4 MB 19.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 171.0/199.4 MB 19.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 171.7/199.4 MB 19.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 172.4/199.4 MB 19.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 173.1/199.4 MB 19.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 173.8/199.4 MB 19.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 174.5/199.4 MB 19.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 175.2/199.4 MB 19.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 175.9/199.4 MB 19.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 176.6/199.4 MB 19.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 177.3/199.4 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 178.1/199.4 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 178.8/199.4 MB 19.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 179.5/199.4 MB 19.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 180.2/199.4 MB 19.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.0/199.4 MB 19.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.7/199.4 MB 19.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 182.4/199.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 183.2/199.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 184.0/199.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 184.8/199.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 185.5/199.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 186.3/199.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 187.0/199.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 187.8/199.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 188.5/199.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 189.0/199.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 189.5/199.4 MB 19.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 190.1/199.4 MB 19.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 190.6/199.4 MB 19.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 191.1/199.4 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 191.7/199.4 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 192.2/199.4 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 192.8/199.4 MB 17.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 193.4/199.4 MB 17.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 194.0/199.4 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  194.6/199.4 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.2/199.4 MB 17.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  195.8/199.4 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  196.4/199.4 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  197.0/199.4 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  197.6/199.4 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.3/199.4 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.8/199.4 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/199.4 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 199.4/199.4 MB 12.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "   ---------------------------------------- 0.0/436.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 436.4/436.4 kB 13.3 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "   ---------------------------------------- 0.0/179.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 179.3/179.3 kB ? eta 0:00:00\n",
      "Downloading regex-2024.9.11-cp310-cp310-win_amd64.whl (274 kB)\n",
      "   ---------------------------------------- 0.0/274.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 274.0/274.0 kB 16.5 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp310-none-win_amd64.whl (285 kB)\n",
      "   ---------------------------------------- 0.0/285.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 285.9/285.9 kB 17.2 MB/s eta 0:00:00\n",
      "Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 16.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.5/1.7 MB 18.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 18.0 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/6.2 MB 23.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 25.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.3/6.2 MB 20.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.1/6.2 MB 21.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.8/6.2 MB 22.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.6/6.2 MB 22.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.4/6.2 MB 21.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 21.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 20.8 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 17.0 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, safetensors, regex, networkx, jinja2, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.9.0 huggingface-hub-0.25.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 regex-2024.9.11 safetensors-0.4.5 sympy-1.13.3 tokenizers-0.19.1 torch-2.4.1 transformers-4.44.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nbconvert 7.16.3 requires beautifulsoup4, which is not installed.\n",
      "nbconvert 7.16.3 requires bleach!=5.0.0, which is not installed.\n",
      "nbconvert 7.16.3 requires defusedxml, which is not installed.\n",
      "nbconvert 7.16.3 requires jupyterlab-pygments, which is not installed.\n",
      "nbconvert 7.16.3 requires nbclient>=0.5.0, which is not installed.\n",
      "nbconvert 7.16.3 requires nbformat>=5.7, which is not installed.\n",
      "nbconvert 7.16.3 requires pandocfilters>=1.4.1, which is not installed.\n",
      "nbconvert 7.16.3 requires tinycss2, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\ncannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\transformers\\modeling_tf_utils.py:34\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\__init__.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\ops.py:33\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m message\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attr_value_pb2\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m full_type_pb2\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:5\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"Generated protocol buffer code.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder \u001b[38;5;28;01mas\u001b[39;00m _builder\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\transformers\\integrations\\integration_utils.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1605\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1608\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\ncannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\transformers\\trainer.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     get_reporting_integration_callbacks,\n\u001b[0;32m     44\u001b[0m     hp_params,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1605\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1608\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\ncannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1591\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1605\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1608\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\ncannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\kamru\\.conda\\envs\\py310\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Custom Dataset class for BERT\n",
    "class HomographDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load BERT Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Function to encode labels to numerical values\n",
    "def encode_labels(labels):\n",
    "    label_map = {label: idx for idx, label in enumerate(set(labels))}\n",
    "    return [label_map[label] for label in labels], label_map\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "def prepare_data(file_paths, tokenizer, max_length):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for file_path, label_name in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            data.extend([line.strip() for line in lines if line.strip()])\n",
    "            labels.extend([label_name] * len(lines))\n",
    "    encoded_labels, label_map = encode_labels(labels)\n",
    "    return data, encoded_labels, label_map\n",
    "\n",
    "# Prepare the data for Dak_Dako (as an example, extend this to other homographs)\n",
    "file_paths_dak_dako = [\n",
    "    (\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\dak_dako\\\\dak.txt\", 'dak'),\n",
    "    (\"C:\\\\Users\\\\kamru\\\\Documents\\\\Homograph\\\\bangla_homograph_tagged\\\\dak_dako\\\\dako.txt\", 'dako')\n",
    "]\n",
    "\n",
    "# Preprocess data\n",
    "texts, labels, label_map = prepare_data(file_paths_dak_dako, tokenizer, max_length=128)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = HomographDataset(X_train, y_train, tokenizer, max_length=128)\n",
    "val_dataset = HomographDataset(X_val, y_val, tokenizer, max_length=128)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_map))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Define a custom function for computing metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "pip uninstall transformers torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
